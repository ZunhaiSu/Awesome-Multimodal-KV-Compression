# Awesome-Multimodal-KV-Compression
|Paper|Conference|Author|Code|
|:---:|:---:|:---:|:---:|
|[LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2406.18139)|EMNLP2024|||
|[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)||||
|[FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework](https://arxiv.org/abs/2503.08461)||||
|[AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](https://arxiv.org/abs/2503.23956)||||
|[Cross-Self KV Cache Pruning for Efficient Vision-Language Inference](https://arxiv.org/abs/2412.04652)||||
|[VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration](https://arxiv.org/abs/2410.23317)||||
|[Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU](https://arxiv.org/abs/2409.09086)||||
|[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/abs/2410.08584)||||
|[Efficient Inference of Vision Instruction-Following Models with Elastic Cache](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_4)||||
|[PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation](https://arxiv.org/abs/2412.03409)||||

# Multimodal-KV-Quantization
|Paper|Conference|Author|Code|
|:---:|:---:|:---:|:---:|
|[AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for Vision-Language Models](https://arxiv.org/abs/2501.15021)||||
|[CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs](https://arxiv.org/abs/2502.14882)||||
|[Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models](https://arxiv.org/abs/2503.16257)||||


|Paper|Conference|Author|Code|
|:---:|:---:|:---:|:---:|
|[A White Paper on Neural Network Quantization](https://arxiv.org/pdf/2106.08295)||||
|[PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization](https://arxiv.org/pdf/2410.05265)||||
|[BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache](https://arxiv.org/abs/2503.18773)||||
